# AI Evals Configuration File

[evaluation]
default_timeout_ms = 30000
max_concurrent_evaluations = 10
results_dir = "./results"
cache_dir = "./cache"

[model]
default_model = "gpt-3.5-turbo"
api_endpoint = "https://api.openai.com/v1"
# api_key should be set via environment variable: AI_EVALS_MODEL_API_KEY
request_timeout_ms = 60000
max_retries = 3

[metrics]
enable_detailed_metrics = true
collection_interval_secs = 60
export_to_file = false
# export_file = "./metrics/metrics.json"

[logging]
level = "info"
structured = false
# file_path = "./logs/evals.log"
console_output = true
