[package]
name = "ai-evals"
version = "0.1.0"
edition = "2021"
description = "AI evaluation framework for testing and benchmarking AI systems"
authors = ["AI Evals Team"]
license = "MIT"
repository = "https://github.com/ai-evals/ai-evals"
keywords = ["ai", "evaluation", "benchmarking", "testing"]
categories = ["development-tools", "science"]

[lib]
name = "ai_evals"
path = "src/lib.rs"

[[bin]]
name = "ai-evals"
path = "src/main.rs"

[dependencies]
# Error Handling
thiserror = "2.0.18"
anyhow = "1.0.101"

# Async Runtime
tokio = { version = "1.49.0", features = ["full"] }
async-trait = "0.1.89"
futures = "0.3.32"

# Serialization
serde = { version = "1.0.228", features = ["derive"] }
serde_json = "1.0.149"

# Configuration
figment = { version = "0.10.19", features = ["toml", "env"] }

# Observability
tracing = "0.1.44"
tracing-subscriber = { version = "0.3.22", features = ["env-filter"] }

# Date & Time
chrono = { version = "0.4.43", features = ["serde"] }

# Validation
validator = { version = "0.20.0", features = ["derive"] }

# Statistics
statrs = "0.18.0"

# UUID
uuid = { version = "1.21.0", features = ["v4", "serde"] }

# HTTP Client
reqwest = { version = "0.13.2", features = ["json"] }

# Command line parsing
clap = { version = "4.5.59", features = ["derive"] }

# URL parsing
url = "2.5.8"

# Internal dependencies
ai-models = { path = "../ai-models" }

[dev-dependencies]
tokio-test = "0.4.5"
pretty_assertions = "1.4.1"
criterion = "0.8.2"
cargo-nextest = "0.9.127"
mockall = "0.14.0"

[[bench]]
name = "eval_bench"
harness = false
